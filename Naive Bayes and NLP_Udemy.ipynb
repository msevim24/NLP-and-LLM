{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b601507d-2049-491b-8438-1c1223e925ee",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#e36288; font-family:newtimeroman; color:#FFF9ED; font-size:175%; text-align:center; border-radius:10px 10px;\">Naive Bayes and Natural Language Processing (NLP)</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54162c6e-489f-4774-9db6-f3d14879c8ed",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## <span style=\" color:#e36288\">Naive Bayes Classifier\n",
    "\n",
    "* The Naïve Bayes classifier is a **supervised machine learning algorithm** that is used for classification tasks such as text classification. They use principles of probability to perform classification tasks.\n",
    "* Bayes Theorem is a probability formula that leverages previously known probabilities to define probability of related events occuring: **P(Y|X) = P(X|Y)P(Y) / P(X)**\n",
    "* **Example:** If you get a smoke alarm detecting a fire, what is the probability that there actually is a dangerous fire?\n",
    "\n",
    "  * **A** and **B** are events (Dangerous Fire and Smoke Alarm Triggered)\n",
    "  * **P(A|B)** is probability of event A given that B is True (Probability of Fire given Smoke Alarm)\n",
    "  * **P(B|A)** is probability of event B given that A is True (Probability of Smoke Alarm given a Dangerous Fire)\n",
    "  * **P(A)** is probability of A occuring (Probabiliity of Fire)\n",
    "  * **P(B)** is probability of B occuring  (Probability of Smoke)\n",
    "\n",
    "* Naïve Bayes is part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category. Unlike discriminative classifiers, like logistic regression, **it does not learn which features are most important to differentiate between classes.**\n",
    "* There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: **all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable.** Since all x features are assumed to be **mutually independent of each other**, it is called \"Naive\" Bayes.\n",
    "* For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers **each of these features to contribute independently to the probability** that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.\n",
    "* There are many **variations of Naive Bayes** model, including:\n",
    "  * Multinomial Naive Bayes\n",
    "  * Gaussian Naive Bayes\n",
    "  * Complement Naive Bayes\n",
    "  * Bernouilli Naive Bayes\n",
    "  * Categorical Naive Bayes\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c57fd-8f24-414c-8aa6-01a63b14577e",
   "metadata": {},
   "source": [
    "# Extracting Features from Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864f04e-682c-4015-a802-a2f52ab254c2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info alert\">\n",
    "\n",
    "## <span style=\" color:#bf2e98\">Part One: Core Concepts on Feature Extraction\n",
    "\n",
    "In this section we'll use basic Python to build a rudimentary NLP system. We'll build a **corpus of documents** (two small text files), create a **vocabulary** from all the words in both documents, and then demonstrate a **Bag of Words** technique to extract features from each document.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0caccb65-f259-4c4a-94a5-87c6dd2e9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783255fa-db97-4da9-963e-73387b892429",
   "metadata": {},
   "source": [
    "### Start with some documents:\n",
    "Let's quickly open the tex files and read them. Keep in mind, you should avoid opening and reading entire files if they are very large, as Python could just display everything depending on how you open the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512de58-c8ff-4e34-87fd-c3c4bc56a0d0",
   "metadata": {},
   "source": [
    "#### Read the entire text as a string: read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd74fbc0-ed61-42fc-bff1-2deef4c50f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "with open(\"One.txt\") as mytext: \n",
    "    a = mytext.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d95ff3-d9eb-4233-97f7-77f89a0ee80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a story about dogs\\nour canine pets\\nDogs are furry animals\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "# here \"\\n\" represents the end of the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd6a7ab-19a9-453d-bbe4-676d14bf257f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a story about dogs\n",
      "our canine pets\n",
      "Dogs are furry animals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to see the lines separately use print\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546d9087-a4fb-45d5-a2ae-1aeabe211238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This story is about surfing\n",
      "Catching waves is fun\n",
      "Surfing is a popular water sport\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read another text document\n",
    "with open('Two.txt') as mytext:\n",
    "    print(mytext.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1433d37-2693-41b4-87c6-30dc98fce023",
   "metadata": {},
   "source": [
    "#### Read each line as a list: readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0563a58a-351a-4974-b1f8-336fc76d0e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a story about dogs\\n',\n",
       " 'our canine pets\\n',\n",
       " 'Dogs are furry animals\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can return it as a list using \"readlines\"\n",
    "with open(\"One.txt\") as mytext: \n",
    "    a = mytext.readlines()\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652e69d-88ec-4f63-ba91-0d4c1180ec7f",
   "metadata": {},
   "source": [
    "#### Read the words separately: split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea10250-c71f-4ad8-9795-a3c9199a6a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"One.txt\") as mytext: \n",
    "    a = mytext.read()\n",
    "\n",
    "a.lower().split() # split words as lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49212eb4-929c-4ea5-ba47-b017d612e3e6",
   "metadata": {},
   "source": [
    "### Building a vocabulary (Creating a \"Bag of Words\")\n",
    "\n",
    "Let's create dictionaries that correspond to unique mappings of the words in the documents. We can begin to think of this as mapping out all the possible words available for all (both) documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e893309e-1530-4085-82ea-0d6e185fb566",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"One.txt\") as mytext:\n",
    "    words_one = mytext.read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be19f5bb-8bf8-44e6-b52e-5ad5115195c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce91dfec-a25e-4f40-a8ac-2c12b6088fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac083f9-da11-4eb0-b3c3-c71c7ce3089b",
   "metadata": {},
   "source": [
    "#### unique words: set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b736c05-2ef6-474f-83ce-2456c80ab502",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_words_one = set(words_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "363fca6e-b559-4b7e-9bdc-c0e140239175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'dogs',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'story',\n",
       " 'this'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_words_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d12d03b-0852-4438-9bda-71615c1c4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat it for the other document\n",
    "\n",
    "with open(\"Two.txt\") as mytext:\n",
    "    words_two = mytext.read().lower().split()\n",
    "    uni_words_two = set(words_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd47bb5-bbfe-4f36-805b-e312e4c88edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'catching',\n",
       " 'fun',\n",
       " 'is',\n",
       " 'popular',\n",
       " 'sport',\n",
       " 'story',\n",
       " 'surfing',\n",
       " 'this',\n",
       " 'water',\n",
       " 'waves'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique words in the Two.txt file\n",
    "uni_words_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78f036-436a-491e-9d8c-dad0119fa433",
   "metadata": {},
   "source": [
    "#### Get all unique words across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09154cf1-cd47-4c3f-9879-e978ebf845c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uni_words = set()\n",
    "all_uni_words.update(uni_words_one)\n",
    "all_uni_words.update(uni_words_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9f1dcd0-b75f-4d7d-a1ae-a1f5d6c36790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'catching',\n",
       " 'dogs',\n",
       " 'fun',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'popular',\n",
       " 'sport',\n",
       " 'story',\n",
       " 'surfing',\n",
       " 'this',\n",
       " 'water',\n",
       " 'waves'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_uni_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f5c630-e06f-4a18-ae00-42fec0bd3635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_uni_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb76f69-9918-4924-b272-20f14b293d3b",
   "metadata": {},
   "source": [
    "#### Assign a number for each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de6a7ed2-5baa-4b9d-8245-e9e04a890db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assign numbers to each unique word\n",
    "\n",
    "full_vocab = dict()\n",
    "i = 0\n",
    "\n",
    "for word in all_uni_words:\n",
    "    full_vocab[word] = i\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28e717d5-03f1-4378-ba74-d7eb3efcb47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'waves': 0,\n",
       " 'about': 1,\n",
       " 'dogs': 2,\n",
       " 'a': 3,\n",
       " 'pets': 4,\n",
       " 'is': 5,\n",
       " 'fun': 6,\n",
       " 'water': 7,\n",
       " 'our': 8,\n",
       " 'popular': 9,\n",
       " 'sport': 10,\n",
       " 'this': 11,\n",
       " 'canine': 12,\n",
       " 'surfing': 13,\n",
       " 'furry': 14,\n",
       " 'are': 15,\n",
       " 'story': 16,\n",
       " 'animals': 17,\n",
       " 'catching': 18}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_vocab \n",
    "# there is no alphabetical order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4356f3ae-06f7-40d0-9d9f-59ea86520c8d",
   "metadata": {},
   "source": [
    "### Bag of Words to Frequency Counts\n",
    "\n",
    "Now that we've encapsulated our \"entire language\" in a dictionary, let's perform **feature extraction** on each of our original documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc8328-3889-4fc6-83ae-35108712c5d8",
   "metadata": {},
   "source": [
    "#### Empty counts per doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5651e65b-c333-46bc-8eb7-05cbd609733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty vector with space for each word in the vocabulary:\n",
    "one_freq = [0]*len(full_vocab)\n",
    "two_freq = [0]*len(full_vocab)\n",
    "all_words = ['']*len(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e6f1315-e759-4ec3-a71a-da07010e2ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01d3d819-71da-449a-a924-ff7544e50983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ca39ffc-b991-47b4-8611-a24b210c709b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82999904-d3f4-4d4d-b77e-faacb5b9e88b",
   "metadata": {},
   "source": [
    "#### Add in counts per word per doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91dc9d56-72d6-416a-b20b-b8bf32a71ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the frequencies of each word in One.txt to our vector\n",
    "with open('One.txt') as f:\n",
    "    one_text = f.read().lower().split()\n",
    "    \n",
    "for word in one_text:\n",
    "    word_ind = full_vocab[word]\n",
    "    one_freq[word_ind]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d0b04c5-c81e-4516-92b6-b1d7f79b0e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq\n",
    "# We can see the repeated words (as 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2bd736b-afef-4faa-ab12-fc3461387022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the second document:\n",
    "with open('Two.txt') as f:\n",
    "    two_text = f.read().lower().split()\n",
    "    \n",
    "for word in two_text:\n",
    "    word_ind = full_vocab[word]\n",
    "    two_freq[word_ind]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a07f4524-deb4-4d3d-af9e-834c874ffb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 0, 3, 1, 1, 0, 1, 1, 1, 0, 2, 0, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8a37abc-46a1-4741-84fa-6f37e68c5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in full_vocab:\n",
    "    word_ind = full_vocab[word]\n",
    "    all_words[word_ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1b54bcd-fdbe-42e6-9098-2e6076b05555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waves',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'a',\n",
       " 'pets',\n",
       " 'is',\n",
       " 'fun',\n",
       " 'water',\n",
       " 'our',\n",
       " 'popular',\n",
       " 'sport',\n",
       " 'this',\n",
       " 'canine',\n",
       " 'surfing',\n",
       " 'furry',\n",
       " 'are',\n",
       " 'story',\n",
       " 'animals',\n",
       " 'catching']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words\n",
    "# these words show the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7dec3-08f1-48e9-a851-b1f17e0c3341",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "045a05a7-8d05-4464-a744-d5048c05103e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waves</th>\n",
       "      <th>about</th>\n",
       "      <th>dogs</th>\n",
       "      <th>a</th>\n",
       "      <th>pets</th>\n",
       "      <th>is</th>\n",
       "      <th>fun</th>\n",
       "      <th>water</th>\n",
       "      <th>our</th>\n",
       "      <th>popular</th>\n",
       "      <th>sport</th>\n",
       "      <th>this</th>\n",
       "      <th>canine</th>\n",
       "      <th>surfing</th>\n",
       "      <th>furry</th>\n",
       "      <th>are</th>\n",
       "      <th>story</th>\n",
       "      <th>animals</th>\n",
       "      <th>catching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   waves  about  dogs  a  pets  is  fun  water  our  popular  sport  this  \\\n",
       "0      0      1     2  1     1   1    0      0    1        0      0     1   \n",
       "1      1      1     0  1     0   3    1      1    0        1      1     1   \n",
       "\n",
       "   canine  surfing  furry  are  story  animals  catching  \n",
       "0       1        0      1    1      1        1         0  \n",
       "1       0        2      0    0      1        0         1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words, their index and repetition in each text\n",
    "bow = pd.DataFrame(data=[one_freq,two_freq],columns=all_words)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d93ffb-fb8d-4855-ba22-326f9fc73373",
   "metadata": {},
   "source": [
    "By comparing the vectors we see that some words are common to both, some appear only in `One.txt`, others only in `Two.txt`. Extending this logic to tens of thousands of documents, we would see the vocabulary dictionary grow to hundreds of thousands of words. Vectors would contain mostly zero values, making them **sparse matrices**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5548b7c-480d-4f03-b057-4c1c38cd69f8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "## <span style=\" color:#e36288\">Important Concepts in NLP\n",
    "\n",
    "### Bag of Words and Tf-idf\n",
    "In the above examples, each vector can be considered a **bag of words**. By itself these may not be helpful until we consider **term frequencies**, or how often individual words appear in documents. However, it may be hard to differentiate documents based on term frequency if a word shows up in a majority of documents. To handle this we also consider **inverse document frequency**, which is the total number of documents divided by the number of documents that contain the word. In practice we convert this value to a logarithmic scale, as described [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency).\n",
    "\n",
    "**Term Frequency (TF):** It is the raw count of a term in a document. The number of times that term **t** occurs in a documnet **d**. A simple way to calculate term frequencies is to divide the number of occurrences of a word by the total number of words in the document. In this way, the number of times a word appears in large documents can be compared to that of smaller documents.\n",
    "\n",
    "**Inverse Document Frequency (IDF):** An IDF factor is incorparated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. The closer it is to 0, the more common a word is.\n",
    "* TF-IDF = term frequency * (1 / document frequency), or\n",
    "* TF-IDF = term frequency * inverse document frequency\n",
    "\n",
    "Together these terms become [**tf-idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "### Stop Words and Word Stems\n",
    "**Stop Words** are words common enough throughout a language that it is usually safe to remove them and not consider them as important (e.g. the, a, is, etc.). NLP Libraries have a built-in list of common stop words. Also, it may make sense to only record the **root of a word**, say `cat` in place of both `cat` and `cats`. This will shrink our vocab array and improve performance.\n",
    "\n",
    "### Tokenization and Tagging\n",
    "When we created our vectors the first thing we did was split the incoming text on whitespace with `.split()`. This was a crude form of **tokenization** - that is, dividing a document into individual words. In this simple example we didn't worry about punctuation or different parts of speech. In the real world we rely on some fairly sophisticated *morphology* to parse text appropriately.\n",
    "\n",
    "Once the text is divided, we can go back and **tag** our tokens with information about parts of speech, grammatical dependencies, etc. This adds more dimensions to our data and enables a deeper understanding of the context of specific documents. For this reason, vectors become ***high dimensional sparse matrices***.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b01ec-e538-4014-9f61-3f2bc5d08e5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info alert\">\n",
    "\n",
    "## <span style=\" color:#bf2e98\">Part Two:  Feature Extraction with Scikit-Learn\n",
    "\n",
    "Let's explore the more realistic process of using sklearn to complete the tasks mentioned above!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06669f73-a6d4-4f4a-b3b2-a993031497b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple text consisting of both unique and repetitive words\n",
    "text = ['This is a line',\n",
    "         \"This is another line\",\n",
    "       \"Completely different line\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1955e2-cea7-48cd-8137-738da49fe2a9",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e93e87d9-ddfe-401d-ad50-f3928acfbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "393fe189-1c05-475f-9025-4be77a17cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "310046a0-7e99-4761-8922-6eabd3ef511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ff2099d-667f-4d44-8d8c-17418452f713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform\n",
    "cv.fit_transform(text) # text data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fc6585e-5dcb-4edb-85fe-2d2a57618651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 1, 1, 1],\n",
       "        [1, 0, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can assign it to a variable ans see the sparse matrix (3x6)\n",
    "sparse_matrix = cv.fit_transform(text) \n",
    "sparse_matrix.todense()\n",
    "\n",
    "# each line represents the line of the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d37b5ff6-c204-47bd-b786-8c62e11c3b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'line': 4, 'another': 0, 'completely': 1, 'different': 2}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let^s see the unique words (with their index) in the dictionary\n",
    "cv.vocabulary_\n",
    "\n",
    "# notice that it did not count \"a\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9ed4c-1360-44c8-ad0b-0c9e95cf6da8",
   "metadata": {},
   "source": [
    "#### stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab9bb74b-b71f-4a3b-bc98-d1a560ef6a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try it again using \"stop_words\" argument ans see the matrix\n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "sparse_matrix = cv.fit_transform(text) \n",
    "sparse_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e9f1116-a712-47bd-9437-ef28c0fc6934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line': 2, 'completely': 0, 'different': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c59202-483b-4553-be9e-5d2efaf19079",
   "metadata": {},
   "source": [
    "**Result:** There are only three words. It removed other words such as \"this\", \"another\" and \"is\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1ec3d-392f-4ad6-a07e-c9abb819b039",
   "metadata": {},
   "source": [
    "## TfidfTransformer\n",
    "\n",
    "TfidfVectorizer is used on sentences, while TfidfTransformer is used on an existing count matrix, such as one returned by CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a3c7584-b826-4b70-b5b5-5b350dbabcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "092c7caa-d764-4d96-8af6-aaebd4293436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First fit and transform CountVectorizer()\n",
    "counts = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae406b56-7d65-40b0-bb1d-c48cf2c8ade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9296d8af-f0c3-4163-963f-4d8125841008",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6610eae-261a-464e-83a9-e7e8b3e2bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, fit and transform TfidfTransformer()\n",
    "# BOW --> TF-IDF\n",
    "tfidf= tfidf.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27095c31-d27a-4fc0-8baa-40c881d4857b",
   "metadata": {},
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b5e2ea4-8c5e-40b6-8d6a-e44ec7e39887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c4eb2-3e6c-4322-b108-75ea836a8a98",
   "metadata": {},
   "source": [
    "#### pipeline\n",
    "We can use pipeline for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d729e9e-f711-4d35-aa81-faa5417b2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29799c20-59f8-4116-b42d-92daea631f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cv',CountVectorizer()),('tfidf',TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a450ed8-cd00-4452-8ac6-ee02890fc4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipe.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2529915-0075-414d-b03e-eaa346138ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "601d5e63-6d72-4098-a870-25b5d6ea75f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da6d1d-4cc7-42c6-99a6-cf2533b3ffc7",
   "metadata": {},
   "source": [
    "## TfIdfVectorizer\n",
    "Does both above in a single step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbbad1b4-8da3-4624-a72a-3f6ec170aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF and Vectorizer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c377cee9-757c-46bf-af4a-a68d9eba2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_results = tfidf.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8e0f60c-47ea-4159-a011-df6137c6c242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_results.todense()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
